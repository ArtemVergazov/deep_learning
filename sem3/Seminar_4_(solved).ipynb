{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Seminar 4 (solved).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpVRB4G1V-XT"
      },
      "source": [
        "# Seminar 4: Transfer Learning\n",
        "\n",
        "Today's theory in two pictures:\n",
        "\n",
        "![transfer learning meme](https://user-images.githubusercontent.com/9570420/113611523-522e0180-9657-11eb-919a-639c398740b1.png)\n",
        "![transfer learning diagram](https://user-images.githubusercontent.com/9570420/113617617-362e5e00-965f-11eb-9eeb-f433e780dd00.png)\n",
        "\n",
        "Now to the practice!\n",
        "\n",
        "Transfer learning from a large dataset is almost always better than training from scratch on a small dataset directly. It prevents overfitting and increases accuracy. We will review three methods of transfer learning.\n",
        "\n",
        "The plan:\n",
        "\n",
        "1. Review our target task, i.e. small dataset — telling apart architectural heritage elements on images.\n",
        "2. Review our (far more complex) source task — 1000-class [ImageNet](http://image-net.org/) classification. Download a CNN pretrained on it.\n",
        "3. Use that CNN as a black box feature extractor. Namely, train a classical ML model on outputs of CNN's next to last layer to solve the target task.\n",
        "4. Retrain CNN's last layers only for the target task.\n",
        "5. Train (*fine-tune*) the whole network for the target task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xyzy0MJmV-XV"
      },
      "source": [
        "*(service code)* Import libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCMfMIHLV-XY"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "import torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meuwryMbV-Xr"
      },
      "source": [
        "### 1. Architectural Heritage Elements Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFs8qdLEV-Xt"
      },
      "source": [
        "*(service code)* Download the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2Vo7mKSV-Xu"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "DATASET_ROOT = Path(\"Architectural_Heritage_Elements_Dataset_128(creative_commons)\")\n",
        "!pip install -U gdown\n",
        "\n",
        "if not (DATASET_ROOT / \"train\").is_dir():\n",
        "    !gdown --id 1X38ZkQnox_k0BaYtMlApFmT7oxAK_c9F\n",
        "\n",
        "    ARCHIVE_NAME = Path(\"architectural_heritage_elements.zip\")\n",
        "    assert ARCHIVE_NAME.is_file(), f\"Couldn't download {ARCHIVE_NAME} doesn't exist\"\n",
        "\n",
        "    print(\"Unzipping...\")\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile(ARCHIVE_NAME, 'r') as archive:\n",
        "        archive.extractall(\".\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELniTbubV-X5"
      },
      "source": [
        "*(service code)* Split the dataset into training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WBNfJcQV-X7"
      },
      "source": [
        "import random\n",
        "random.seed(666)\n",
        "\n",
        "num_val_images_per_label = 120\n",
        "\n",
        "if not (DATASET_ROOT / 'val').is_dir():\n",
        "    labels = sorted(x.name for x in DATASET_ROOT.iterdir())\n",
        "\n",
        "    for label in labels:\n",
        "        for split in 'train', 'val':\n",
        "            (DATASET_ROOT / split / label).mkdir(parents=True)\n",
        "\n",
        "        images = sorted((DATASET_ROOT / label).iterdir())\n",
        "        random.shuffle(images)\n",
        "\n",
        "        val_images   = images[:num_val_images_per_label]\n",
        "        train_images = images[num_val_images_per_label:]\n",
        "        \n",
        "        for split, images in ('train', train_images), ('val', val_images):\n",
        "            for image in images:\n",
        "                image.rename(DATASET_ROOT / split / label / image.name)\n",
        "                \n",
        "        (DATASET_ROOT / label).rmdir()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw4xqR_lDz9k"
      },
      "source": [
        "Now we have images grouped in folders by train/val split and then by class, like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef_d3VTTDynL"
      },
      "source": [
        "!ls \"Architectural_Heritage_Elements_Dataset_128(creative_commons)/train/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su214j_XV-YC"
      },
      "source": [
        "`torchvision.datasets.ImageFolder` is a handy class to load images from disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGiZr4WbV-YD"
      },
      "source": [
        "train_dataset = torchvision.datasets.ImageFolder(DATASET_ROOT / 'train')\n",
        "val_dataset   = torchvision.datasets.ImageFolder(DATASET_ROOT / 'val')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cnNDCrwV-YK"
      },
      "source": [
        "`ImageFolder` is a special case of a very important class `torch.utils.data.Dataset`.\n",
        "\n",
        "Any `Dataset` supports `len()` and indexing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOlx_OnlV-YL"
      },
      "source": [
        "print(f\"There are {len(train_dataset)} training images\")\n",
        "print(f\"There are {len(  val_dataset)} validation images\")\n",
        "\n",
        "image, label = train_dataset[50]\n",
        "print(f\"Dataset returns {type(image)} and {type(label)}\\n\")\n",
        "\n",
        "class_names = train_dataset.classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOOAby_UV-YU"
      },
      "source": [
        "Let's explore the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YG0ku7RvV-YV"
      },
      "source": [
        "image, label = train_dataset[2020]\n",
        "print(class_names[label])\n",
        "print(\"Image size:\", image.size)\n",
        "\n",
        "image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8JzJ3O6V-Yc"
      },
      "source": [
        "### 2. CNNs pretrained on the *ImageNet* dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IPd2p1wV-Yd"
      },
      "source": [
        "#### 2.1 The ImageNet challenge\n",
        "\n",
        "In ImageNet, there are ~1M images of 1000 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgnIIR4oV-Yf"
      },
      "source": [
        "*(service code)* Download the class labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2p99xHcrV-Yg"
      },
      "source": [
        "import urllib.request\n",
        "import json\n",
        "\n",
        "LABELS_URL = 'https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json'\n",
        "with urllib.request.urlopen(LABELS_URL) as f:\n",
        "    imagenet_class_names = json.loads(f.read().decode())\n",
        "\n",
        "print(f\"There are {len(imagenet_class_names)} classes in ImageNet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VF1TgFA0V-Yq"
      },
      "source": [
        "Explore those classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrAh69-5V-Yr"
      },
      "source": [
        "import random\n",
        "\n",
        "print(\"10 random ImageNet classes:\\n\")\n",
        "for _ in range(10):\n",
        "    class_idx = random.randint(0, len(imagenet_class_names))\n",
        "    print(\"Class %d: %s\" % (class_idx, imagenet_class_names[class_idx]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOTwNFkuV-Yx"
      },
      "source": [
        "#### 2.2 Pretrained CNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFCOCM1uV-Yz"
      },
      "source": [
        "`torchvision.models` [provides](https://pytorch.org/vision/stable/models.html) pretrained CNNs, including those trained on ImageNet.\n",
        "\n",
        "Let's use for example the \"[SqueezeNet 1.1](https://github.com/pytorch/vision/blob/master/torchvision/models/squeezenet.py)\" CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA7S79BGV-Y5"
      },
      "source": [
        "??torchvision.models.squeezenet1_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "sOH7Io1OV-Y_"
      },
      "source": [
        "<img src=\"https://cdn-images-1.medium.com/max/800/1*xji5NAhX6m3Nk7BmR_9GFw.png\" width=650>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tWE6GoLV-ZB"
      },
      "source": [
        "Download pretrained weights and create the CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbVAe_dQV-ZB",
        "scrolled": false
      },
      "source": [
        "model = torchvision.models.squeezenet1_1(\n",
        "    pretrained=True,\n",
        "    num_classes=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xghN1eZeV-ZH"
      },
      "source": [
        "* If you only want to evaluate the model, call `.eval()`.\n",
        "* If you want to train the model, call `.train()`.\n",
        "\n",
        "This is needed because some layers — like dropout or batch normalization — behave differently during training and testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPUDsoxGV-ZH"
      },
      "source": [
        "model.eval();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akYfF6CkV-ZP"
      },
      "source": [
        "Also, if you don't train (i.e. don't need gradients with respect to parameters), call `.requires_grad_(False)` to save memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85Y9Q72UV-ZR"
      },
      "source": [
        "model.requires_grad_(False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SghsN6TSV-ZV"
      },
      "source": [
        "First, let's try to forward some garbage through the model.\n",
        "\n",
        "[According to the docs](https://pytorch.org/vision/stable/models.html), we will have to use inputs of size at least $224 \\times 224$ pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxbkPSoXV-ZW"
      },
      "source": [
        "# A mini-batch of 5 random \"images\" of size 234 x 345\n",
        "sample_input = torch.randn(5, 3, 234, 345)\n",
        "sample_output = model(sample_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-N6cU4eV-Zc"
      },
      "source": [
        "This CNN outputs class scores (not probabilities yet).\n",
        "\n",
        "The higher the $i$-th score, the higher the CNN's confidence that this image is of $i$-th class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBNePU1pV-Ze"
      },
      "source": [
        "print(\n",
        "    f\"The CNN has returned a tensor of {sample_output.shape} \"\n",
        "    f\"with values from {sample_output.min().item()} to {sample_output.max().item()}\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGGLViapV-Zj"
      },
      "source": [
        "Now time to forward some real images through the CNN.\n",
        "\n",
        "Before that, they have to be normalized [as per the same documentation](https://pytorch.org/vision/stable/models.html):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adJNuGzDV-Zj"
      },
      "source": [
        "normalize = torchvision.transforms.Normalize(\n",
        "    mean=[0.485, 0.456, 0.406],\n",
        "    std=[0.229, 0.224, 0.225])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5VOcQj-V-Zo"
      },
      "source": [
        "import PIL.Image\n",
        "\n",
        "def predict(image):\n",
        "    \"\"\"\n",
        "    image:\n",
        "        PIL.Image\n",
        "\n",
        "    return:\n",
        "        tensor, shape == (1000,)\n",
        "        ImageNet class probabilities.\n",
        "    \"\"\"\n",
        "    # Scale the image to 224x224 pixels.\n",
        "    # In fact, the images can safely be larger or not square\n",
        "    image = image.resize((224, 224))\n",
        "\n",
        "    # Convert `PIL.Image` to `torch.tensor` and normalize\n",
        "    image = torchvision.transforms.ToTensor()(image)\n",
        "    image = normalize(image)\n",
        "    \n",
        "    # Add singleton batch dimension with [None]\n",
        "    image = image[None]\n",
        "    \n",
        "    # Predict class scores\n",
        "    prediction = model(image)\n",
        "    \n",
        "    # Convert class scores to probabilities\n",
        "    prediction = prediction.softmax(1)\n",
        "    \n",
        "    # Remove singleton batch dimension\n",
        "    return prediction[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHC1Xwb8V-Zs"
      },
      "source": [
        "Try to classify some real images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qO3SzqLV-Zs"
      },
      "source": [
        "import urllib\n",
        "\n",
        "IMAGE_URL = \"http://qph.fs.quoracdn.net/main-qimg-3c711f9ad560f1703125f2839f5d8ad6\"\n",
        "# IMAGE_URL = \"https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/cute-photos-of-cats-by-bowl-1593202944.jpg?crop=0.668xw:1.00xh;0.216xw,0&resize=224:*\"\n",
        "# IMAGE_URL = \"https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/cute-photos-of-cats-in-box-1593184776.jpg?crop=0.6666666666666666xw:1xh;center,top&resize=224:*\"\n",
        "# IMAGE_URL = \"https://i.pinimg.com/originals/23/dd/90/23dd903fc557cfbc95f0176c24829c95.jpg\"\n",
        "# IMAGE_URL = \"https://i.pinimg.com/474x/6e/d3/0c/6ed30c47246083096f0d9b79e122dace.jpg\"\n",
        "\n",
        "with urllib.request.urlopen(IMAGE_URL) as f:\n",
        "    image = PIL.Image.open(f)\n",
        "\n",
        "image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lhmv8mYsV-Zx"
      },
      "source": [
        "probabilities = predict(image)\n",
        "\n",
        "top_probabilities, top_indices = torch.topk(probabilities, 10)\n",
        "print(\"10 most probable classes are:\")\n",
        "for probability, class_idx in zip(top_probabilities, top_indices):\n",
        "    print(\"%.4f: %s\" % (probability, imagenet_class_names[class_idx]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B73IxZmEV-Z1"
      },
      "source": [
        "### 3. Use classical machine learning with a pretrained CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPGb6yVHV-Z2"
      },
      "source": [
        "Our network is some convolutional part `model.features` followed by a classification block `model.classifier` (you learn this by reading model's `forward()` in its [source code](https://github.com/pytorch/vision/blob/master/torchvision/models/squeezenet.py))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvCgC3SKbQJp"
      },
      "source": [
        "model.features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "linAKAnwV-Z3"
      },
      "source": [
        "`model.features` outputs a stack of 512 uninterpretable feature maps, of size $20 \\times 20$ in this case:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUDj46MxV-Z7"
      },
      "source": [
        "sample_input = torch.randn(5, 3, 333, 333)\n",
        "model.features(sample_input).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dxV9LRWV-aB"
      },
      "source": [
        "Which are then passed through this `model.classifier` block:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8KPDCPuV-aB"
      },
      "source": [
        "model.classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBq0p3-7V-aF"
      },
      "source": [
        "Although the outputs of `model.features` are uninterpretable, they contain a lot of useful information about input images. They are called *latent encodings* or *latent embeddings* of the input images, or *intermediate features* (hence the name `model.features`).\n",
        "\n",
        "Here is an example visualization of these features ([source](https://www.researchgate.net/figure/t-SNE-embedding-of-a-subset-of-the-convolutional-features-extracted-from-Imagenet_fig1_283762714)):\n",
        "\n",
        "![](https://www.researchgate.net/profile/Zhongfei_Zhang/publication/283762714/figure/fig1/AS:614293271756808@1523470337103/t-SNE-embedding-of-a-subset-of-the-convolutional-features-extracted-from-Imagenet.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z49M_TqLV-aG"
      },
      "source": [
        "These features can be sufficient to recognize architectural elements in images. Let's train a random forest classifier on them.\n",
        "\n",
        "Sometimes it's called \"using a CNN as a *black-box feature extractor*\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8-V-y8HV-aH"
      },
      "source": [
        "First, implement image preprocessing.\n",
        "\n",
        "Images have to be of same size ($224 \\times 224$ in our case) for efficiency: we will stack them in tensors of size $B \\times 3 \\times 224 \\times 224$ representing mini-batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzbxRscqV-aM"
      },
      "source": [
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((224, 224)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "train_dataset_224 = torchvision.datasets.ImageFolder(DATASET_ROOT / 'train', transform)\n",
        "val_dataset_224   = torchvision.datasets.ImageFolder(DATASET_ROOT / 'val'  , transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl7uqbwbV-aP"
      },
      "source": [
        "image, _ = train_dataset_224[66]\n",
        "print(f\"Datasets now return images as tensors of size {image.shape} and datatype {image.dtype}\")\n",
        "\n",
        "assert \\\n",
        "    torch.is_tensor(image) and \\\n",
        "    image.dtype == torch.float32 and \\\n",
        "    image.shape == (3, 224, 224)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ5m0muqV-aY"
      },
      "source": [
        "For even better efficiency, we use `torch.utils.data.DataLoader` class. It is a wrapper around `torch.utils.data.Dataset` that automatically loads data in the background and groups them into batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zz_g6mLIV-aZ"
      },
      "source": [
        "??torch.utils.data.DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxp4-vWvV-ac"
      },
      "source": [
        "BATCH_SIZE = 24\n",
        "NUM_WORKERS = 3\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset_224, batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS, shuffle=True, pin_memory=True)\n",
        "\n",
        "val_dataloader = torch.utils.data.DataLoader(\n",
        "    val_dataset_224, batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS, shuffle=False, pin_memory=True)\n",
        "\n",
        "print(f\"Train/val dataloaders have {len(train_dataloader)} and {len(val_dataloader)} batches\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWs23LHzV-ae"
      },
      "source": [
        "Upload the model to GPU (if it's available)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdpuva4tV-af"
      },
      "source": [
        "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD7R47ltV-ai"
      },
      "source": [
        "model.to(DEVICE);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xsus-fuxV-al"
      },
      "source": [
        "Compute embeddings for the whole dataset.\n",
        "\n",
        "Also, let's average the features over spatial dimensions (i.e. apply *global average pooling*). We do this because we don't care in which part of an image a tower/dome was, but rather \"how much of a tower/dome\" there was.\n",
        "\n",
        "This way we encode every image with a **512-dimensional vector**. (in the image below, $d$ could be 512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcwGgljwV-am"
      },
      "source": [
        "![image](data:image/jpeg;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCGtJ9VcV-an"
      },
      "source": [
        "def compute_embeddings(dataloader):\n",
        "    \"\"\"\n",
        "    Compute latent embeddings for all images in `dataloader` using `model` CNN.\n",
        "    \n",
        "    dataloader:\n",
        "        torch.utils.data.DataLoader\n",
        "    \n",
        "    return:\n",
        "    X:\n",
        "        tensor, shape: (len(dataloader.dataset), 512), dtype: float32, device: 'cpu'\n",
        "    y:\n",
        "        tensor, shape: (len(dataloader.dataset),)    , dtype: int64  , device: 'cpu'\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    X, y = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader):\n",
        "            embeddings = model.features(images.to(DEVICE)).mean([2, 3])\n",
        "\n",
        "            X.append(embeddings.cpu())\n",
        "            y.append(labels)\n",
        "\n",
        "    return torch.cat(X), torch.cat(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SPbMfLXV-aq"
      },
      "source": [
        "X_train, y_train = compute_embeddings(train_dataloader)\n",
        "X_val  , y_val   = compute_embeddings(  val_dataloader)\n",
        "\n",
        "assert X_train.shape == (len(train_dataloader.dataset), 512)\n",
        "assert y_train.shape == (len(train_dataloader.dataset),)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-NeDAXHV-as"
      },
      "source": [
        "Train a random forest classifier on the embeddings that we just precomputed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FA8sNGxQV-at"
      },
      "source": [
        "import sklearn.ensemble\n",
        "import sklearn.metrics\n",
        "\n",
        "classifier = sklearn.ensemble.RandomForestClassifier()\n",
        "classifier.fit(X_train, y_train);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY-K57yvV-av"
      },
      "source": [
        "train_accuracy = sklearn.metrics.accuracy_score(y_train, classifier.predict(X_train))\n",
        "val_accuracy   = sklearn.metrics.accuracy_score(y_val  , classifier.predict(X_val))\n",
        "\n",
        "print(f\"Train accuracy: {train_accuracy * 100:.2f}%\")\n",
        "print(f\"Validation accuracy: {val_accuracy * 100:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G19aIo67V-ay"
      },
      "source": [
        "### 4. Replace the last layers and retrain them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_y8IzdtV-az"
      },
      "source": [
        "The above approach isn't very convenient because we have to run a separate algorithm.\n",
        "\n",
        "It's more convenient to add a neural network on top of `model.features` instead of a random forest. This way, our pipeline becomes **end-to-end**: a single `torch.nn.Module` directly outputs the prediction we need (the probabilities of \"tower/dome/...\" classes).\n",
        "\n",
        "Still, don't expect the accuracy to be much higher than with random forest. We just replaced it with a simple two-layer neural net, nothing more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfMSMHqvV-a0"
      },
      "source": [
        "class RetrofittedCNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        pretrained_net = torchvision.models.squeezenet1_1(\n",
        "            pretrained=True,\n",
        "            num_classes=1000)\n",
        "        self.CNN = pretrained_net.features\n",
        "\n",
        "        self.classifier = torch.nn.Sequential(\n",
        "            torch.nn.Dropout(0.5),\n",
        "            torch.nn.Linear(512, 512),\n",
        "            torch.nn.BatchNorm1d(512),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(0.5),\n",
        "            torch.nn.Linear(512, len(class_names))\n",
        "        )\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\"\n",
        "        Define the forward pass.\n",
        "        \"\"\"\n",
        "        features = self.CNN(images).mean([2, 3])\n",
        "        class_scores = self.classifier(features)\n",
        "        return class_scores\n",
        "\n",
        "torch.manual_seed(666) # for reproducibility\n",
        "model = RetrofittedCNN().to(DEVICE)\n",
        "model.CNN.requires_grad_(False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT9JAfhhV-a2"
      },
      "source": [
        "Set the optimization parameters.\n",
        "\n",
        "Note that we only want to train the classifier!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDYfeV5UV-a3"
      },
      "source": [
        "trainable_parameters = model.classifier.parameters()\n",
        "\n",
        "learning_rate = 3e-3\n",
        "optimizer = torch.optim.Adam(\n",
        "    trainable_parameters, lr=learning_rate, weight_decay=1e-4)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gzRURp4V-a7"
      },
      "source": [
        "Describe the training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvwE6xdVV-a8"
      },
      "source": [
        "def validate(model, dataloader):\n",
        "    \"\"\"Compute accuracy on the `dataloader` dataset.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            labels = labels.to(DEVICE)\n",
        "            dog_probabilities = model(images.to(DEVICE))\n",
        "            predictions = dog_probabilities.max(1)[1]\n",
        "\n",
        "            total += len(labels)\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            \n",
        "    return correct / total\n",
        "\n",
        "def train(model, dataloader, criterion, optimizer):\n",
        "    \"\"\"Train for one epoch, return accuracy and average loss.\"\"\"\n",
        "    model.train()\n",
        "\n",
        "    correct, total = 0, 0\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for images, labels in tqdm(dataloader):\n",
        "        probabilities = model(images.to(DEVICE))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            labels = labels.to(DEVICE)\n",
        "            predictions = probabilities.max(1)[1]\n",
        "            \n",
        "            total += len(labels)\n",
        "            correct += (predictions == labels).sum().item()\n",
        "\n",
        "        loss_value = criterion(probabilities, labels)\n",
        "        total_loss += loss_value.item() * len(labels)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss_value.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    return correct / total, total_loss / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiC5V4AlV-a-"
      },
      "source": [
        "def set_learning_rate(optimizer, new_learning_rate):\n",
        "    \"\"\"Set learning rates of the optimizer to `new_learning_rate`.\"\"\"\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = new_learning_rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KA52hmG1V-bB"
      },
      "source": [
        "epochs = 0\n",
        "train_accuracy, val_accuracy, train_loss = float('nan'), float('nan'), float('nan')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpEChH0rV-bI"
      },
      "source": [
        "for _ in range(18):\n",
        "    # Compute validation accuracy\n",
        "    val_accuracy = validate(model, val_dataloader)\n",
        "    print(\n",
        "        f\"After {epochs} epochs, training accuracy: {train_accuracy * 100:.2f}%\"\n",
        "        f\" (loss {train_loss:.4f}), validation accuracy: {val_accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Train for one epoch\n",
        "    train_accuracy, train_loss = train(model, train_dataloader, criterion, optimizer)\n",
        "    epochs += 1\n",
        "\n",
        "    # Decrease learning rate sometimes\n",
        "    if epochs in (4, 8, 12):\n",
        "        learning_rate /= 10\n",
        "        set_learning_rate(optimizer, learning_rate)\n",
        "        print(f\"Decreasing the learning rate to {learning_rate}\")\n",
        "\n",
        "# Compute final validation accuracy\n",
        "val_accuracy = validate(model, val_dataloader)\n",
        "print(\n",
        "    f\"After {epochs} epochs, training accuracy: {train_accuracy * 100:.2f}%\"\n",
        "    f\" (loss {train_loss:.4f}), validation accuracy: {val_accuracy * 100:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD52GOx-BOCg"
      },
      "source": [
        "It looks like the neural network classifier generalizes better than random forest. Anyway, they both are limited by the \"frozen\" CNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knrXXJbIV-bL"
      },
      "source": [
        "### 5. Fine-tune the whole network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKsZwF1mV-bM"
      },
      "source": [
        "We have seen that `model.features` is already a good feature extractor. Still, it's not aware of our dataset.\n",
        "\n",
        "Let's train **all** layers (including our new retrofitted classifier) to get extra accuracy. This process is called **fine-tuning**.\n",
        "\n",
        "Because the original `model.features`' weights are valuable, fragile and co-adapted, fine-tuning requires several precautions:\n",
        "\n",
        "* If the retrofitted classifier's weights are random, it will propagate large gradients into the pre-trained CNN. That can wreck the pre-trained weights. That's why one should either\n",
        "  * train the retrofitted classifier alone for a while (**we've just done this**); or\n",
        "  * assign a larger learning rate to the classifier and a smaller one to pre-trained weights (e.g. $10^{-2}$ and $10^{-4}$ with SGD); or\n",
        "  * (an option for the lazy) train everything with a reduced learning rate (e.g. $10^{-3}$).\n",
        "* Use a less aggressive optimizer, e.g. momentum SGD, Adam with warm-up (**we do this below**), or RAdam.\n",
        "* Use a smaller learning rate than usually. **Below I used $\\leq 10^{-4}$**, while on random weights I'd use something around $10^{-3}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds4bMGGr_X5U"
      },
      "source": [
        "# Don't forget to turn gradient computing back on; \"unfreeze\" the CNN\n",
        "model.CNN.requires_grad_(True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ejfzn-n_WWC"
      },
      "source": [
        "Here, we set zero learning rate to warm Adam up.\n",
        "\n",
        "Warm-up means running some iterations without any weight updates, just to let Adam accumulate gradient statistics. Otherwise, its first optimization steps will be very noisy and destructive for the pre-trained weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjET1PwvV-bR"
      },
      "source": [
        "# Now the parameters have to include both CNN's and classifier's weights\n",
        "trainable_parameters = model.parameters()\n",
        "\n",
        "# Note the zero learning rate\n",
        "learning_rate = 0\n",
        "optimizer = torch.optim.Adam(\n",
        "    trainable_parameters, lr=learning_rate, weight_decay=1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpotA1JWARSM"
      },
      "source": [
        "Run 1 epoch of warm-up.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eBHgkhGvVc7"
      },
      "source": [
        "train(model, train_dataloader, criterion, optimizer);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNi9vgfaAYzh"
      },
      "source": [
        "Switch back to a positive learning rate. Remember, it should be smaller than usual (above, we used $3 \\cdot 10^{-3}$)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKcrlYGh5n8r"
      },
      "source": [
        "learning_rate = 8e-5\n",
        "set_learning_rate(optimizer, learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16LLTW_vAxSo"
      },
      "source": [
        "Finally, fine-tune the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEauyylhV-bV"
      },
      "source": [
        "epochs = 0\n",
        "train_accuracy, val_accuracy, train_loss = float('nan'), float('nan'), float('nan')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_j2w6NGV-bZ"
      },
      "source": [
        "for _ in range(15):\n",
        "    # Compute validation accuracy\n",
        "    val_accuracy = validate(model, val_dataloader)\n",
        "    print(\n",
        "        f\"After {epochs} epochs, training accuracy: {train_accuracy * 100:.2f}%\"\n",
        "        f\" (loss {train_loss:.4f}), validation accuracy: {val_accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Train for one epoch\n",
        "    train_accuracy, train_loss = train(model, train_dataloader, criterion, optimizer)\n",
        "    epochs += 1\n",
        "\n",
        "    # Decrease learning rate sometimes\n",
        "    if epochs in (7, 12):\n",
        "        learning_rate /= 10\n",
        "        set_learning_rate(optimizer, learning_rate)\n",
        "        print(f\"Decreasing the learning rate to {learning_rate}\")\n",
        "\n",
        "val_accuracy = validate(model, val_dataloader)\n",
        "print(\n",
        "    f\"After {epochs} epochs, training accuracy: {train_accuracy * 100:.2f}%\"\n",
        "    f\" (loss {train_loss:.4f}), validation accuracy: {val_accuracy * 100:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBaVCi6_V-be"
      },
      "source": [
        "We decreased the validation error by a couple of percent, which is great!\n",
        "\n",
        "Also, note how the training loss was stuck in the previous run, but started to go to zero after we gave model extra capacity by \"unfreezing\" the pre-trained weights.\n",
        "\n",
        "## 6. Conclusions\n",
        "\n",
        "* Fine-tuning is an extremely important technique. It's ubiquitous in deep learning. Make sure to master it.\n",
        "* Almost all of you will need it in final course projects.\n",
        "* Starting from pre-trained weights is almost always better than starting from random ones,\n",
        "* ...and is actually the only solution when your dataset is small.\n",
        "* Fine-tuning has to be done very carefully, with proper intuition and commonly known tricks (see above)."
      ]
    }
  ]
}