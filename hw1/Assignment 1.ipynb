{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "gFe273vKFgF-"
   },
   "source": [
    "# Assignment 1 (60 points total)\n",
    "\n",
    "You will train a convolutional neural network (aka ConvNet or CNN) to solve yet another image classification problem: the Tiny ImageNet dataset (200 classes, 100K training images, 10K validation images). Try to achieve as high accuracy as possible.\n",
    "\n",
    "This exercise is close to what people do in real life. No toy architectures this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-S-WyYK8FgGF"
   },
   "source": [
    "## Grading\n",
    "\n",
    "* 11 points for the report.\n",
    "* 5 points for using an **interactive** (don't reinvent the wheel with `plt.plot`) tool for viewing progress, for example TensorBoard.\n",
    "* 9 points for a network that gets $\\geq$25% accuracy on the private **test** set.\n",
    "* Up to 35 points for accuracy up to 50%, issued linearly (i.e. 0 points for 25%, 7 points for 30%, 21 points for 40%, 35 points for $\\geq$50%.\n",
    "\n",
    "## Grading Explained\n",
    "\n",
    "* *Private test set*: it's a part of the dataset like the validation set, but for which the ground truth labels are known only to us (you won't be able to evaluate your model on it). When grading, we will compute test accuracy by running your code that computes val accuracy, but having replaced the images in `'val/'` with the test set.\n",
    "* *How to submit*:\n",
    "  * **<font color=\"red\">Read this in advance, don't leave until the last minute.</font> Wrong checkpoint submission = <font color=\"red\">0 points for accuracy</font>. Be careful!**\n",
    "  * After you've trained your network, [save weights](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html) to \"*checkpoint.pth*\" with `model.state_dict()` and `torch.save()`.\n",
    "  * Set `DO_TRAIN = False`, click \"Restart and Run All\" and make sure that your validation accuracy is computed correctly.\n",
    "  * Compute the MD5 checksum for \"*checkpoint.pth*\" (e.g. run `!md5sum checkpoint.pth`) and paste it into \"*solution.py*\" (`get_checkpoint_metadata()`). You'll be penalized if this checksum doesn't match your submitted file.\n",
    "  * Upload \"*checkpoint.pth*\" to Google Drive, copy the view-only link to it and paste it into \"*solution.py*\" as well.\n",
    "  * Make sure \"Restart and Run All\" also works with `DO_TRAIN = True`: trains your model and computes validation accuracy.\n",
    "  * <font color=\"red\">Important</font>: At least several hours before the deadline, **upload \"*solution.py*\" [here](http://350e-83-69-192-100.ngrok.io/) and make sure you get a \"👌\"**.\n",
    "\n",
    "* *Report*: PDF, free form; should mention:\n",
    "  * Your history of tweaks and improvements. How you started, what you searched. (*I have analyzed these and those conference papers/sources/blog posts. I tried this and that to adapt them to my problem. ...*)\n",
    "  * Which network architectures have you tried? Which of them didn't work, and can you guess why? What is the final one and why?\n",
    "  * Same for the training method (batch size, optimization algorithm, number of iterations, ...): which and why?\n",
    "  * Same for anti-overfitting (regularization) techniques. Which ones have you tried? What were their effects, and can you guess why?\n",
    "  * **Most importantly**: deep learning insights you gained. Can you give several examples of how *exactly* experience from this exercise will affect you training your future neural nets? (tricks, heuristics, conclusions, observations)\n",
    "  * **List all sources of code**.\n",
    "* *Progress viewing tool*: support the report with screenshots of accuracy and loss plots (training and validation) over time.\n",
    "\n",
    "## Restrictions\n",
    "\n",
    "* No pretrained networks.\n",
    "* Don't enlarge images (e.g. don't resize them to $224 \\times 224$ or $256 \\times 256$).\n",
    "\n",
    "## Tips\n",
    "\n",
    "* **One change at a time**: don't test several new things at once (unless you are super confident that they will work). Train a model, introduce one change, train again.\n",
    "* Google a lot: try to reinvent as few wheels as possible. Harvest inspiration from PyTorch recipes, from GitHub, from blogs...\n",
    "* Use GPU.\n",
    "* Regularization is very important: L2, batch normalization, dropout, data augmentation...\n",
    "* Pay much attention to accuracy and loss graphs (e.g. in TensorBoard). Track failures early, stop bad experiments early.\n",
    "* 2-3 hours of training (in Colab) should be enough for most models, maybe 4-6 hours if you're experimenting.\n",
    "* Save checkpoints every so often in case things go wrong (optimization diverges, Colab disconnects...).\n",
    "* Don't use too large batches, they can be slow and memory-hungry. This is true for inference too.\n",
    "* Also don't forget to use `torch.no_grad()` and `.eval()` during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16098,
     "status": "ok",
     "timestamp": 1649386097624,
     "user": {
      "displayName": "Artem Vergazov",
      "userId": "17672996270211226436"
     },
     "user_tz": -180
    },
    "id": "tFM8olIJBrnF",
    "outputId": "6863bd6a-559c-4897-ff4b-8ba7f453fd6b"
   },
   "outputs": [],
   "source": [
    "# Determine the locations of auxiliary libraries and datasets.\n",
    "# `AUX_DATA_ROOT` is where 'tiny-imagenet-2022.zip' is.\n",
    "\n",
    "# Detect if we are in Google Colaboratory\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "from pathlib import Path\n",
    "if IN_COLAB:\n",
    "    google.colab.drive.mount(\"/content/drive\")\n",
    "    \n",
    "    # Change this if you created the shortcut in a different location\n",
    "    AUX_DATA_ROOT = Path(\"/content/drive/My Drive/Deep Learning 2022 -- Home Assignment 1\")\n",
    "    \n",
    "    assert AUX_DATA_ROOT.is_dir(), \"Have you forgot to 'Add a shortcut to Drive'?\"\n",
    "    \n",
    "    import sys\n",
    "    sys.path.append(str(AUX_DATA_ROOT))\n",
    "else:\n",
    "    AUX_DATA_ROOT = Path(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "TcFGHHXVsM-J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "# Your solution\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%aimport solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "X5eKWQv9wi3P"
   },
   "outputs": [],
   "source": [
    "# If `True`, will train the model from scratch and validate it.\n",
    "# If `False`, instead of training will load weights from './checkpoint.pth'.\n",
    "# When grading, we will test both cases.\n",
    "DO_TRAIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "njk50aDoFgGT"
   },
   "outputs": [],
   "source": [
    "# Put training and validation images in `./tiny-imagenet-200/train` and `./tiny-imagenet-200/val`:\n",
    "if not Path(\"tiny-imagenet-200/train/class_000/00000.jpg\").is_file():\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(AUX_DATA_ROOT / 'tiny-imagenet-2022.zip', 'r') as archive:\n",
    "        archive.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AFqnb1-EFgGj"
   },
   "outputs": [],
   "source": [
    "# Initialize dataloaders\n",
    "train_dataloader = solution.get_dataloader(\"./tiny-imagenet-200/\", 'train')\n",
    "val_dataloader   = solution.get_dataloader(\"./tiny-imagenet-200/\", 'val')\n",
    "\n",
    "# Initialize the raw model\n",
    "model = solution.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "J9n7DyGcFgGq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in c:\\users\\kuzne\\anaconda3\\envs\\sci-dev\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kuzne\\anaconda3\\envs\\sci-dev\\lib\\site-packages (from gdown) (4.62.3)\n",
      "Requirement already satisfied: six in c:\\users\\kuzne\\anaconda3\\envs\\sci-dev\\lib\\site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\kuzne\\anaconda3\\envs\\sci-dev\\lib\\site-packages (from gdown) (4.10.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kuzne\\anaconda3\\envs\\sci-dev\\lib\\site-packages (from gdown) (3.4.0)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\kuzne\\anaconda3\\envs\\sci-dev\\lib\\site-packages (from gdown) (2.26.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\kuzne\\anaconda3\\envs\\sci-dev\\lib\\site-packages (from beautifulsoup4->gdown) (2.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kuzne\\anaconda3\\envs\\sci-dev\\lib\\site-packages (from requests[socks]->gdown) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kuzne\\anaconda3\\envs\\sci-dev\\lib\\site-packages (from requests[socks]->gdown) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\kuzne\\anaconda3\\envs\\sci-dev\\lib\\site-packages (from requests[socks]->gdown) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kuzne\\anaconda3\\envs\\sci-dev\\lib\\site-packages (from requests[socks]->gdown) (3.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\kuzne\\anaconda3\\envs\\sci-dev\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\kuzne\\anaconda3\\envs\\sci-dev\\lib\\site-packages (from tqdm->gdown) (0.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kuzne\\anaconda3\\envs\\sci-dev\\lib\\site-packages\\gdown\\cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1eAf16xpXCQJbZVoSobmQ8No_eidoVhKs\n",
      "To: d:\\docs\\Git\\skoltech\\term4\\dl\\hw1\\checkpoint.pth\n",
      "\n",
      "  0%|          | 0.00/45.2M [00:00<?, ?B/s]\n",
      "  1%|1         | 524k/45.2M [00:00<00:09, 4.76MB/s]\n",
      "  5%|4         | 2.10M/45.2M [00:00<00:04, 8.72MB/s]\n",
      "  8%|8         | 3.67M/45.2M [00:00<00:03, 10.5MB/s]\n",
      " 12%|#1        | 5.24M/45.2M [00:00<00:03, 11.0MB/s]\n",
      " 15%|#5        | 6.82M/45.2M [00:00<00:03, 11.3MB/s]\n",
      " 19%|#8        | 8.39M/45.2M [00:00<00:03, 11.4MB/s]\n",
      " 22%|##2       | 9.96M/45.2M [00:00<00:03, 11.5MB/s]\n",
      " 26%|##5       | 11.5M/45.2M [00:01<00:02, 11.6MB/s]\n",
      " 29%|##9       | 13.1M/45.2M [00:01<00:02, 11.7MB/s]\n",
      " 32%|###2      | 14.7M/45.2M [00:01<00:02, 11.7MB/s]\n",
      " 36%|###5      | 16.3M/45.2M [00:01<00:02, 11.7MB/s]\n",
      " 39%|###9      | 17.8M/45.2M [00:01<00:02, 11.2MB/s]\n",
      " 43%|####2     | 19.4M/45.2M [00:01<00:02, 11.5MB/s]\n",
      " 46%|####6     | 21.0M/45.2M [00:01<00:02, 11.9MB/s]\n",
      " 50%|####9     | 22.5M/45.2M [00:01<00:01, 11.9MB/s]\n",
      " 53%|#####3    | 24.1M/45.2M [00:02<00:01, 11.9MB/s]\n",
      " 57%|#####6    | 25.7M/45.2M [00:02<00:01, 11.8MB/s]\n",
      " 60%|######    | 27.3M/45.2M [00:02<00:01, 11.8MB/s]\n",
      " 64%|######3   | 28.8M/45.2M [00:02<00:01, 11.8MB/s]\n",
      " 67%|######7   | 30.4M/45.2M [00:02<00:01, 11.8MB/s]\n",
      " 71%|#######   | 32.0M/45.2M [00:02<00:01, 11.8MB/s]\n",
      " 74%|#######4  | 33.6M/45.2M [00:02<00:01, 11.6MB/s]\n",
      " 78%|#######7  | 35.1M/45.2M [00:03<00:00, 11.8MB/s]\n",
      " 81%|########1 | 36.7M/45.2M [00:03<00:00, 11.8MB/s]\n",
      " 85%|########4 | 38.3M/45.2M [00:03<00:00, 11.8MB/s]\n",
      " 88%|########8 | 39.8M/45.2M [00:03<00:00, 11.7MB/s]\n",
      " 92%|#########1| 41.4M/45.2M [00:03<00:00, 11.8MB/s]\n",
      " 95%|#########5| 43.0M/45.2M [00:03<00:00, 11.8MB/s]\n",
      " 99%|#########8| 44.6M/45.2M [00:03<00:00, 11.8MB/s]\n",
      "100%|##########| 45.2M/45.2M [00:03<00:00, 11.6MB/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] Не удается найти указанный файл",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1656/156527294.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# Compute the actual checksum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         real_md5_checksum = subprocess.check_output(\n\u001b[0m\u001b[0;32m     22\u001b[0m             [\"md5sum\", \"checkpoint.pth\"]).decode().split()[0]\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sci-dev\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[1;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0m\u001b[0;32m    425\u001b[0m                **kwargs).stdout\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sci-dev\\lib\\subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    503\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stderr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPIPE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 505\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    506\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m             \u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sci-dev\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[0;32m    949\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[0;32m    950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 951\u001b[1;33m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[0;32m    952\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sci-dev\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1418\u001b[0m             \u001b[1;31m# Start the process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1419\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m                 hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n\u001b[0m\u001b[0;32m   1421\u001b[0m                                          \u001b[1;31m# no special security\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m                                          \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] Не удается найти указанный файл"
     ]
    }
   ],
   "source": [
    "if DO_TRAIN:\n",
    "    # Train from scratch\n",
    "    optimizer = solution.get_optimizer(model)\n",
    "    solution.train_on_tinyimagenet(train_dataloader, val_dataloader, model, optimizer)\n",
    "else:\n",
    "    # Download the checkpoint and initialize model weights from it\n",
    "    import urllib\n",
    "    import subprocess\n",
    "\n",
    "    penalize = False\n",
    "\n",
    "    # Get your link and checksum\n",
    "    claimed_md5_checksum, google_drive_link = solution.get_checkpoint_metadata()\n",
    "\n",
    "    # Use your link to download \"checkpoint.pth\"\n",
    "    !pip install -U gdown\n",
    "    !gdown --id {urllib.parse.urlparse(google_drive_link).path.split('/')[-2]} -O checkpoint.pth\n",
    "\n",
    "    try:\n",
    "        # Compute the actual checksum\n",
    "        real_md5_checksum = subprocess.check_output(\n",
    "            [\"md5sum\", \"checkpoint.pth\"]).decode().split()[0]\n",
    "    except subprocess.CalledProcessError as err:\n",
    "        # Couldn't download or the filename isn't \"checkpoint.pth\"\n",
    "        print(f\"Wrong link or filename: {err}\")\n",
    "        penalize = True\n",
    "    else:\n",
    "        # The trained checkpoint is different from the one submitted\n",
    "        if real_md5_checksum != claimed_md5_checksum:\n",
    "            print(\"Checksums differ! Late submission?\")\n",
    "            penalize = True\n",
    "\n",
    "    if penalize:\n",
    "        print(\"🔫 Prepare the penalizer! 🔫\")\n",
    "\n",
    "    # Finally load weights\n",
    "    solution.load_weights(model, \"./checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CDJw8MokFxP9"
   },
   "outputs": [],
   "source": [
    "# Classify some validation samples\n",
    "import torch\n",
    "\n",
    "example_batch, example_batch_labels = next(iter(val_dataloader))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  _, example_predicted_labels = solution.predict(model, example_batch).max(1)\n",
    "\n",
    "print(\"Predicted class / Ground truth class\")\n",
    "for predicted, gt in list(zip(example_predicted_labels, example_batch_labels))[:15]:\n",
    "    print(\"{:03d} / {:03d}\".format(predicted, gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_Qddecy7-uS"
   },
   "outputs": [],
   "source": [
    "# Print validation accuracy\n",
    "val_accuracy, _ = solution.validate(val_dataloader, model)\n",
    "val_accuracy *= 100\n",
    "assert 1.5 <= val_accuracy <= 100.0\n",
    "print(\"Validation accuracy: %.2f%%\" % val_accuracy)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 1",
   "provenance": [
    {
     "file_id": "1_Yw8KmHcOSyf4tE_ipmzD2_yBXIUPAry",
     "timestamp": 1648550927230
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
